{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "pyspark_tfidf.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMmpeFCqbZ84D6R4tRQmpfO",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jianfeiZhao/BI_projs/blob/master/pyspark_tfidf.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kvfKJiFJPkHB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "outputId": "405f4374-1376-4635-9a46-1dc189df06a9"
      },
      "source": [
        "!pip install pyspark"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pyspark in /usr/local/lib/python3.6/dist-packages (3.0.0)\n",
            "Requirement already satisfied: py4j==0.10.9 in /usr/local/lib/python3.6/dist-packages (from pyspark) (0.10.9)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6x0T-83svJ_9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 237
        },
        "outputId": "2d1fd152-48d6-4754-9f4c-3178c9ca07c5"
      },
      "source": [
        "# 使用pyspark计算文档的TFIDF\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.ml.feature import Tokenizer, HashingTF, IDF\n",
        "\n",
        "# 创建SparkSession，2.0版本之后只需要创建一个SparkSession即可\n",
        "spark=SparkSession \\\n",
        "        .builder \\\n",
        "        .appName('tfidf_app') \\\n",
        "        .getOrCreate()\n",
        "\n",
        "# 加载数据\n",
        "documents = spark.createDataFrame([\n",
        "    (0, \"我 非常 喜欢 看 电视剧\", \"data1\"),\n",
        "    (1, \"我 喜欢 看 电视剧\", \"data2\"),\n",
        "    (2, \"我 喜欢 吃 苹果\",\"data3\"),\n",
        "    (3, \"我 喜欢 吃 苹果 看 电视剧\",\"data4\")], [\"id\", \"doc_text\", \"other\"])\n",
        "\n",
        "# 转化为视图\n",
        "documents.registerTempTable(\"doc_table\")\n",
        "df = spark.sql(\"SELECT id, doc_text FROM doc_table\")\n",
        "print('df=')\n",
        "df.show() # 打印前20行\n",
        "print(df.collect()) # 转化为列表\n",
        "# id\n",
        "#df.select('id').show() "
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "df=\n",
            "+---+-------------------------+\n",
            "| id|                 doc_text|\n",
            "+---+-------------------------+\n",
            "|  0|   我 非常 喜欢 看 电视剧|\n",
            "|  1|        我 喜欢 看 电视剧|\n",
            "|  2|          我 喜欢 吃 苹果|\n",
            "|  3|我 喜欢 吃 苹果 看 电视剧|\n",
            "+---+-------------------------+\n",
            "\n",
            "[Row(id=0, doc_text='我 非常 喜欢 看 电视剧'), Row(id=1, doc_text='我 喜欢 看 电视剧'), Row(id=2, doc_text='我 喜欢 吃 苹果'), Row(id=3, doc_text='我 喜欢 吃 苹果 看 电视剧')]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HVy39ZfOvRvD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 181
        },
        "outputId": "13e1e66d-ce5d-4e76-bd6e-c03cd893b4b4"
      },
      "source": [
        "# 将desc进行分词\n",
        "tokenizer = Tokenizer(inputCol=\"doc_text\", outputCol=\"doc_words\")\n",
        "df = tokenizer.transform(df)\n",
        "df.show()"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+---+-------------------------+----------------------------+\n",
            "| id|                 doc_text|                   doc_words|\n",
            "+---+-------------------------+----------------------------+\n",
            "|  0|   我 非常 喜欢 看 电视剧|[我, 非常, 喜欢, 看, 电视剧]|\n",
            "|  1|        我 喜欢 看 电视剧|      [我, 喜欢, 看, 电视剧]|\n",
            "|  2|          我 喜欢 吃 苹果|        [我, 喜欢, 吃, 苹果]|\n",
            "|  3|我 喜欢 吃 苹果 看 电视剧| [我, 喜欢, 吃, 苹果, 看,...|\n",
            "+---+-------------------------+----------------------------+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "56ItjadoQtNp",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 237
        },
        "outputId": "6aed85e4-f303-4696-8cb4-0acaf1b2faa1"
      },
      "source": [
        "# 计算每篇文档的TF-IDF\n",
        "hashingTF = HashingTF(inputCol='doc_words', outputCol=\"doc_words_tf\")\n",
        "tf = hashingTF.transform(df).cache()\n",
        "idf = IDF(inputCol='doc_words_tf', outputCol=\"doc_words_tfidf\").fit(tf)\n",
        "tfidf = idf.transform(tf).cache()\n",
        "print('\\n 每个文档的TFIDF')\n",
        "tfidf.select('doc_words_tfidf').show(truncate=False)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            " 每个文档的TFIDF\n",
            "+----------------------------------------------------------------------------------------------------------------------------------------+\n",
            "|doc_words_tfidf                                                                                                                         |\n",
            "+----------------------------------------------------------------------------------------------------------------------------------------+\n",
            "|(262144,[62541,75305,96624,163839,227575],[0.22314355131420976,0.0,0.0,0.9162907318741551,0.22314355131420976])                         |\n",
            "|(262144,[62541,75305,96624,227575],[0.22314355131420976,0.0,0.0,0.22314355131420976])                                                   |\n",
            "|(262144,[75305,95584,96624,230496],[0.0,0.5108256237659907,0.0,0.5108256237659907])                                                     |\n",
            "|(262144,[62541,75305,95584,96624,227575,230496],[0.22314355131420976,0.0,0.5108256237659907,0.0,0.22314355131420976,0.5108256237659907])|\n",
            "+----------------------------------------------------------------------------------------------------------------------------------------+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S_vMciKkRVj2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 217
        },
        "outputId": "7f5e3793-4ec8-4fcc-a846-57bc49056e0c"
      },
      "source": [
        "# 数据规范化，默认为2阶范式\n",
        "from pyspark.ml.feature import Normalizer\n",
        "normalizer = Normalizer(inputCol=\"doc_words_tfidf\", outputCol=\"norm\") #默认.setP(2.0)\n",
        "tfidf = normalizer.transform(tfidf)\n",
        "#tfidf.select('norm').show(truncate=False)\n",
        "\n",
        "import pyspark.sql.functions as psf \n",
        "from pyspark.sql.types import DoubleType\n",
        "dot_udf = psf.udf(lambda x,y: float(x.dot(y)), DoubleType())\n",
        "tfidf.alias(\"a1\").join(tfidf.alias(\"a2\"), psf.col(\"a1.id\") < psf.col(\"a2.id\"))\\\n",
        "    .select(\n",
        "        psf.col(\"a1.id\").alias(\"id1\"), \n",
        "        psf.col(\"a2.id\").alias(\"id2\"), \n",
        "        dot_udf(\"a1.norm\", \"a2.norm\").alias(\"dot\"))\\\n",
        "    .sort(\"id1\", \"id2\")\\\n",
        "    .show()"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+---+---+-------------------+\n",
            "|id1|id2|                dot|\n",
            "+---+---+-------------------+\n",
            "|  0|  1|0.32563128587164397|\n",
            "|  0|  2|                0.0|\n",
            "|  0|  3| 0.1303511225242502|\n",
            "|  1|  2|                0.0|\n",
            "|  1|  3| 0.4003028215649755|\n",
            "|  2|  3| 0.9163829172606391|\n",
            "+---+---+-------------------+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WgiFJjSXS2zt",
        "colab_type": "text"
      },
      "source": [
        "上表显示的是各个句子之间的相似度结果。"
      ]
    }
  ]
}
